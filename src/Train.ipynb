{
 "cells": [],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [
     "import pandas as pd\n",
     "import matplotlib.pyplot as plt\n",
     "import numpy as np\n",
     "from sklearn.metrics import mean_squared_error\n",
     "import keras\n",
     "import tensorflow as tf\n",
     "import json\n",
     "import time\n",
     "from keras.preprocessing.sequence import TimeseriesGenerator\n",
     "from keras.models import Sequential, model_from_json\n",
     "from keras.layers import LSTM, Dense\n",
     "from keras import optimizers\n",
     "\n",
     "from sklearn import datasets\n",
     "import pickle\n",
     "import time\n",
     "import pymongo\n",
     "\n",
     "\n",
     "class Predict:\n",
     "\n",
     "    def __init__(self):\n",
     "        # data_name = \"XOM\"\n",
     "        data_name = \"PETR4_SA_1\"\n",
     "        look_back = 15\n",
     "        epochs_num = 1\n",
     "        # switch_key = [False, False]  # [train, test]\n",
     "        switch_key = [True, True]  # [train, test]\n",
     "        # switch_key = [True, False]  # [train, test]\n",
     "        # switch_key = [True, True] #[train, test]\n",
     "\n",
     "        inicio = time.time()\n",
     "\n",
     "        # test/train just one time\n",
     "        self.NewMethod(epochs_num, data_name, look_back, switch_key)\n",
     "\n",
     "        #test/train an array\n",
     "        # epochs_arrays = [20, 30, 50, 100, 200, 300, 400, 500]\n",
     "        # for ep in epochs_arrays:\n",
     "        #     if ep == 100:\n",
     "        #         switch_key = [False, True]\n",
     "        #     else:\n",
     "        #         switch_key = [True, True]\n",
     "        #     self.NewMethod(ep, data_name, look_back, switch_key)\n",
     "\n",
     "\n",
     "        fim = time.time()\n",
     "        tempo_total = (fim - inicio) / 60\n",
     "        print(\"Tempo de execução foi de: %d minutos\" % tempo_total)\n",
     "        \n",
     "    def NewMethod(self, epochs_num, data_name, look_back, switch_key):\n",
     "        # --------------------------------TRAINING PART--------------------------------\n",
     "        print(\"inicio do treio\")\n",
     "        df = pd.read_csv('./../Data/{}.csv'.format(data_name))\n",
     "        #print(df.info())\n",
     "\n",
     "        # Getting only Date and Close columns\n",
     "        df['Date'] = pd.to_datetime(df['Date'])\n",
     "        df.set_axis(df['Date'], inplace=True)\n",
     "        df.drop(columns=['Open', 'High', 'Low', 'Volume'], inplace=True)\n",
     "\n",
     "        \"\"\"\n",
     "        #Plot close value\n",
     "        df['Close'].plot(figsize=(16, 6), label='GOOG Close')\n",
     "        plt.title(\"Test\")\n",
     "        plt.xlabel('Time')\n",
     "        plt.ylabel('GOOG Stock Price')\n",
     "        plt.legend()\n",
     "        plt.show()\n",
     "        \"\"\"\n",
     "\n",
     "        # --------------------------------DATA PROCESSING--------------------------------\n",
     "\n",
     "        # Train the model on the first 80% of data and test it on the remaining 20%.\n",
     "\n",
     "        close_data = df['Close'].values\n",
     "        close_data = close_data.reshape((-1, 1))\n",
     "\n",
     "        split_percent = 0.80\n",
     "        split = int(split_percent * len(close_data))\n",
     "\n",
     "        close_train = close_data[:split]\n",
     "        close_test = close_data[split:]\n",
     "\n",
     "        date_train = df['Date'][:split]\n",
     "        date_test = df['Date'][split:]\n",
     "\n",
     "        #print(len(close_train))\n",
     "        #print(len(close_test))\n",
     "\n",
     "        #Using TimeseriesGenerator to get the time series\n",
     "        train_generator = TimeseriesGenerator(close_train, close_train, length=look_back, batch_size=20)\n",
     "        valid_data_generator = TimeseriesGenerator(close_train, close_train, length=look_back, batch_size=1)\n",
     "        test_generator = TimeseriesGenerator(close_test, close_test, length=look_back, batch_size=1)\n",
     "\n",
     "        # train_generator_array = np.array(train_generator)\n",
     "        # test_generator_arraytest_generator_array = np.array(test_generator)\n",
     "        # print(close_train)\n",
     "        # print(train_generator)\n",
     "\n",
     "        # --------------------------------NEURAL NETWORK--------------------------------\n",
     "        if(switch_key[0] == True):\n",
     "            model = Sequential()\n",
     "            model.add(\n",
     "                LSTM(10,\n",
     "                     # activation = 'SineReLU',\n",
     "                     activation='relu',\n",
     "                     input_shape=(look_back, 1))\n",
     "            )\n",
     "            model.add(Dense(1))\n",
     "            model.compile(optimizer='adam', loss='mse', metrics=['accuracy'])\n",
     "\n",
     "            #model.fit_generator(train_generator, epochs=epochs_num, verbose=2)\n",
     "\n",
     "            #Saving the Neural network\n",
     "            history = model.fit(train_generator, epochs=epochs_num, validation_data=valid_data_generator, verbose=1)\n",
     "            # Get the dictionary containing each metric and the loss for each epoch\n",
     "            history_dict = history.history\n",
     "            # Save it under the form of a json file\n",
     "            json.dump(history_dict,\n",
     "                      open(\"./../Models/{}/json/history-{}-{}.json\".format(data_name, data_name, epochs_num), 'w'))\n",
     "            # Save model\n",
     "            # serialize model to JSON\n",
     "            model_json = model.to_json()\n",
     "            with open(\"./../Models/{}/json/regressor-{}-{}.json\".format(data_name, data_name, epochs_num),\n",
     "                      \"w\") as json_file:\n",
     "                json_file.write(model_json)\n",
     "            # serialize weights to HDF5\n",
     "            model.save_weights(\"./../Models/{}/h5/regressor-{}-{}.h5\".format(data_name, data_name, epochs_num))\n",
     "            print(\"Saved model to disk\")\n",
     "            \n",
     "            details = save_model_to_db(model = xgb, client =pymongo.MongoClient(\"mongodb+srv://pedro:projetos2neuralnetwork@cluster0.wzyex.mongodb.net/myFirstDatabase?retryWrites=true&w=majority\"), db = 'client.test', \n",
     "                dbconnection = 'customers', model_name = 'myxgb')\n",
     "\n",
     "        # --------------------------------TESTING PART--------------------------------\n",
     "    def save_model_to_db(model, client, db, dbconnection, model_name):\n",
     "        print(\"Iniciar envio para o banco de dados\")\n",
     "        #pickling the model\n",
     "        pickled_model = pickle.dumps(model)\n",
     "\n",
     "        #saving model to mongoDB\n",
     "        # creating connection\n",
     "        myclient = pymongo.MongoClient(client)\n",
     "\n",
     "        #creating database in mongodb\n",
     "        mydb = myclient[db]\n",
     "\n",
     "        #creating collection\n",
     "        mycon = mydb[dbconnection]\n",
     "        info = mycon.insert_one({model_name: pickled_model, 'name': model_name, 'created_time':time.time()})\n",
     "        print(info.inserted_id, ' saved with this id successfully!')\n",
     "\n",
     "        details = {\n",
     "            'inserted_id':info.inserted_id,\n",
     "            'model_name':model_name,\n",
     "            'created_time':time.time()\n",
     "        }\n",
     "        print(\"envio concluido\")\n",
     "        return details\n",
     "        \n",
     "start = Predict()\n",
     "\n"
    ],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}